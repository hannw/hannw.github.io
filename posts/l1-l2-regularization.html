<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hann | L1, L2 regularization demystified.</title>
  <meta name="description" content="Breaking down L1, L2 regularization.">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta property="og:title" content="L1, L2 regularization demystified.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="hannw.github.io/posts/l1-l2-regularization">
  <meta property="og:description" content="Breaking down L1, L2 regularization.">
  <meta property="og:site_name" content="Hann">
  <meta property="og:image" content="hannw.github.io/assets/og-image.jpg">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="hannw.github.io/posts/l1-l2-regularization">
  <meta name="twitter:title" content="L1, L2 regularization demystified.">
  <meta name="twitter:description" content="Breaking down L1, L2 regularization.">
  <meta name="twitter:image" content="hannw.github.io/assets/og-image.jpg">

  <link rel="apple-touch-icon" href="/assets/apple-touch-icon.png">
  <link href="hannw.github.io/feed.xml" type="application/rss+xml" rel="alternate" title="Hann Last 10 blog posts" />

  

  
    <link type="text/css" rel="stylesheet" href="/assets/light.css">
  
</head>

<body>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>if (typeof MathJax != "undefined" && typeof MathJax.Hub != "undefined") 
                    MathJax.Hub.Queue(['Typeset', MathJax.Hub]);</script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function () {
    document.getElementById("hide_page").style.visibility = "";
  });
</script>

<div id="hide_page" style="visibility:hidden">
  <main role="main">
    <div class="grid grid-centered">
      <div class="grid-cell">
        <nav class="header-nav reveal">
  <a href="/" class="header-logo" title="Hann">Hann <small>AI, ML, Data Science</small></a>
  <ul class="header-links">
    
      <li>
        <a href="/about" title="About me">
          <span class="icon icon-android-person"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://github.com/hannw" target="_blank" title="GitHub">
          <span class="icon icon-social-github"></span>
        </a>
      </li>
    
    
    
    
      <li>
        <a href="https://www.linkedin.com/in/hann-wang-4951442b" target="_blank" title="LinkedIn">
          <span class="icon icon-social-linkedin"></span>
        </a>
      </li>
    
    
    
      <li>
        <a href="mailto:lostkingwwwth@gmail.com" title="Email">
          <span class="icon icon-at"></span>
        </a>
      </li>
    
    
  </ul>
</nav>

        <article class="article reveal">
          <header class="article-header">
            <h1>L1, L2 regularization demystified.</h1>
            <p>Breaking down L1, L2 regularization.</p>
            <div class="article-list-footer">
              <span class="article-list-date">
                April 14, 2017
              </span>
              <span class="article-list-divider">-</span>
              <span class="article-list-minutes">
                
                
                  3 minute read
                
              </span>
              <span class="article-list-divider">-</span>
              <div class="article-list-tags">
                
                  <a href="/tag/regularization">regularization</a>
                
                  <a href="/tag/machine learning">machine learning</a>
                
              </div>
            </div>
          </header>

          <div class="article-content">
            <p>I decided to write about L1, L2 regularization out of frustration in finding intuitive explanation online. It’s probably buried deep inside some statistics or machine learning courses, but if you only have 5 minutes, and about to have your next nerve wrecking data science job interview, then you are doomed.</p>

<p>Let me head straight to the conclusion now. You will get sparsity in your parameters with L1, but the exact opposite with L2, period. Everyone who has been through enough interview can give you the answer. But why is that the case? It is not obvious just by looking at the formula.</p>

<p>Let’s say you want to find the optimal solution to a regression problem to a general, nonlinear function \(f\) by minimizing the following formulae. In the case of L1,</p>

<script type="math/tex; mode=display">\text{Loss}_{\text(L1)} =  \text{MSE}(D, \mathbf{w}) + \text{L1}(\mathbf{w}) =  \sum_{i=1}^{n} (f(x_i;\mathbf{w}) - y_i)^2 + \lambda \sum_{j=1}^k |w_j|.</script>

<p>And in the case of L2,</p>

<script type="math/tex; mode=display">\text{Loss}_{\text(L2)} =  \text{MSE}(D, \mathbf{w}) + \text{L1}(\mathbf{w}) =  \sum_{i=1}^{n} (f(x_i;\mathbf{w}) - y_i)^2 + \lambda \sum_{j=1}^k w_j^2.</script>

<p>It is not immediately obvious why L1 gives you sparsity of \(\mathbf{w}\). The answer will become clear once we consider the error surfaces of the two terms in the loss formula seperately in the parameter space. First, let’s consider the contour of the L1 error surface; this is where only the \(|\mathbf{w}|\) term is presented.</p>

<script type="math/tex; mode=display">g(w) = \sum_{j=1}^k |w_j|.</script>

<p><a href="/assets/L1L2/L1_contour.png" class="fluidbox-trigger">
  <img src="/assets/L1L2/L1_contour.png" alt="L1 contour" />
</a></p>

<p>The contour shows up diamond-shaped, and the diamond grows larger as the error increases. Each line is called the L1 circle. Why is it called “circle”? Imagine you are an alien who can only measure distance of a point to the origin by using a tool perpendicular to the axes and summing the measurements, then every point in the line will have equal distance to the center. The L1 function, \(g\) defines exactly the distance to center.</p>

<p>Similarly, the L2 equation will give us our regular “circle”, as the formula defines exactly the euclidean distance.</p>

<p><a href="/assets/L1L2/L2_contour.png" class="fluidbox-trigger">
  <img src="/assets/L1L2/L2_contour.png" alt="L2 contour" />
</a></p>

<p>Now, what happens when you have another competing error term? We can check in the following figure; the error surface (for instance, MSE) is colored in orange red. The regularization term and the error term are competing because when you optimize the loss you are finding a point in the parameter space where the sum of the two terms are the minimum. When one losses, the other grows, so the terms are basically “competing” for a position during optimization. If you change the regularization “strength”, \(\lambda\), the cometition changes, and the global optimum settle to a different point.</p>

<p>The sparsity of the L1 term comes from the fact that the diamond shape has a higher chance of settling to any of the corners during optimization. In the 2d case, finding optimum in the corner means that one of the parameter is zero. Imaging in multidimensional case, the corners will be hyper corners, where only some of the parameters are zero. These corners are the cause of the sparsity during optimization.</p>

<p><a href="/assets/L1L2/L1_error_contour.png" class="fluidbox-trigger">
  <img src="/assets/L1L2/L1_error_contour.png" alt="L1 contour" />
</a></p>

<p>If you look at the L2 surface, the cometition changes. The L2 surface does not favor the corner; L2 can settle at any angle of the circle, making all of the weight terms non-zero.</p>

<p><a href="/assets/L1L2/L2_error_contour.png" class="fluidbox-trigger">
  <img src="/assets/L1L2/L2_error_contour.png" alt="L1 contour" />
</a></p>

<p>So, if you want to do feature selection, go with L1 regularization. The exact opposite applies to L2; that is, L2 will tend to preserve all the terms in your parameters. As some say they tend to see performance boost using L2; they are most likely overfitting by memorizing their data in the parameter space.</p>

<p>Some useful links.</p>
<ol>
  <li><a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">Wiki - Regularization</a></li>
  <li><a href="https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization">Quora thread on L1 L2</a></li>
</ol>

          </div>

          <div class="article-share">
            
            <a href="" title="Share on Twitter" onclick="window.open('https://twitter.com/home?status=L1, L2 regularization demystified. - hannw.github.io/posts/l1-l2-regularization ', 'newwindow', 'width=500, height=225'); return false;" data-turbolinks="false">
              <svg enable-background="new 0 0 128 128" width="15px" version="1.1" viewBox="0 0 128 128" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="_x37__stroke"><g id="Twitter"><rect clip-rule="evenodd" fill="none" fill-rule="evenodd" height="128" width="128"/><path clip-rule="evenodd" d="M128,23.294    c-4.703,2.142-9.767,3.59-15.079,4.237c5.424-3.328,9.587-8.606,11.548-14.892c-5.079,3.082-10.691,5.324-16.687,6.526    c-4.778-5.231-11.608-8.498-19.166-8.498c-14.493,0-26.251,12.057-26.251,26.927c0,2.111,0.225,4.16,0.676,6.133    C41.217,42.601,21.871,31.892,8.91,15.582c-2.261,3.991-3.554,8.621-3.554,13.552c0,9.338,4.636,17.581,11.683,22.412    c-4.297-0.131-8.355-1.356-11.901-3.359v0.331c0,13.051,9.053,23.937,21.074,26.403c-2.201,0.632-4.523,0.948-6.92,0.948    c-1.69,0-3.343-0.162-4.944-0.478c3.343,10.694,13.035,18.483,24.53,18.691c-8.986,7.227-20.315,11.533-32.614,11.533    c-2.119,0-4.215-0.123-6.266-0.37c11.623,7.627,25.432,12.088,40.255,12.088c48.309,0,74.717-41.026,74.717-76.612    c0-1.171-0.023-2.342-0.068-3.49C120.036,33.433,124.491,28.695,128,23.294" fill-rule="evenodd" id="Twitter_1_"/></g></g></svg>
            </a>
            <a href="" title="Share on Facebook" onclick="window.open('https://www.facebook.com/sharer/sharer.php?u=hannw.github.io/posts/l1-l2-regularization', 'newwindow', 'width=500, height=500'); return false;" data-turbolinks="false">
              <svg enable-background="new 0 0 128 128" width="15px" version="1.1" viewBox="0 0 128 128" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="_x31__stroke"><g id="Facebook_1_"><rect fill="none" height="128" width="128"/><path clip-rule="evenodd" d="M68.369,128H7.065C3.162,128,0,124.836,0,120.935    V7.065C0,3.162,3.162,0,7.065,0h113.871C124.837,0,128,3.162,128,7.065v113.87c0,3.902-3.163,7.065-7.064,7.065H88.318V78.431    h16.638l2.491-19.318H88.318V46.78c0-5.593,1.553-9.404,9.573-9.404l10.229-0.004V20.094c-1.769-0.235-7.841-0.761-14.906-0.761    c-14.749,0-24.846,9.003-24.846,25.535v14.246H51.688v19.318h16.681V128z" fill-rule="evenodd" id="Facebook"/></g></g></svg>
            </a>
            <a href="" title="Share on Google+" onclick="window.open('https://plus.google.com/share?url=hannw.github.io/posts/l1-l2-regularization', 'newwindow', 'width=550, height=400'); return false;" data-turbolinks="false">
              <svg enable-background="new 0 0 128 128" version="1.1" viewBox="0 0 128 128" width="20px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="_x35__stroke"><g id="Google_Plus"><rect clip-rule="evenodd" fill="none" fill-rule="evenodd" height="128" width="128"/><path clip-rule="evenodd" d="M40.654,55.935v16.13    c0,0,15.619-0.021,21.979-0.021C59.189,82.5,53.834,88.194,40.654,88.194c-13.338,0-23.748-10.832-23.748-24.194    s10.41-24.194,23.748-24.194c7.052,0,11.607,2.483,15.784,5.944c3.344-3.35,3.065-3.828,11.573-11.877    c-7.222-6.586-16.822-10.6-27.357-10.6C18.201,23.273,0,41.507,0,64c0,22.493,18.201,40.727,40.654,40.727    c33.561,0,41.763-29.275,39.044-48.792H40.654z M113.912,56.742V42.628h-10.063v14.113H89.358v10.081h14.491v14.517h10.063V66.823    H128V56.742H113.912z" fill-rule="evenodd" id="Google_Plus_1_"/></g></g></svg>
            </a>
          </div>

          
            <div id="disqus_thread" class="article-comments"></div>
            <script>
              (function() {
                  var d = document, s = d.createElement('script');
                  s.src = '//hannw-github-io.disqus.com/embed.js';
                  s.setAttribute('data-timestamp', +new Date());
                  (d.head || d.body).appendChild(s);
              })();
            </script>
            <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
          
        </article>
        <footer class="footer reveal">
  <p>
    &copy 2018 Hann Wang. All rights reserved. Powered by <a href="https://github.com/nielsenramon/chalk" target="_blank" title="Download Chalk">Chalk</a>.
    <!-- Chalk is a high quality, completely customizable, performant and 100% free
    blog template for Jekyll built by
    <a href="/about" title="About me">Nielsen Ramon</a>. Download it <a href="https://github.com/nielsenramon/chalk" target="_blank" title="Download Chalk">here</a>. -->
  </p>
</footer>

      </div>
    </div>
  </main>
  
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.16/webfont.js"></script>
  <script>
    WebFont.load({
      google: {
        families: ['Cormorant Garamond:700', 'Lato:300,400,700']
      }
    });
  </script>



  <script>
    window.ga=function(){ga.q.push(arguments)};ga.q=[];ga.l=+new Date;
    ga('create','UA-113253527-1','auto');ga('send','pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>


<!-- <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
</script> -->

<script type="text/javascript" src="/assets/vendor.js"></script>
<script type="text/javascript" src="/assets/application.js"></script>

</div>
</body>
</html>
