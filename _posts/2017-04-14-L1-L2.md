---
layout: post
title: "L1, L2 regularization."
description: "Breaking down L1, L2 regularization."
tags: [regularization, machinelearning]
---

I decided to write about L1, L2 regularization out of frustration in finding intuitive explanation online. It probabily lives somewhere buried deep inside certain statistics or machine learning courses, but if you only have 5 minutes, and happen to have an interview for your next nerve wrecking data science job interview, then you are doomed.

Let me get straight to the conclution first. You will get sparsity in your parameters with L1, but the exact opposite with L2, period. Now, everyone who has been through enough interview can give you the answer. But why is that the case? It is not obvious just by looking at the formula. 

Under construction.
